{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FM0quKoSjwp"
      },
      "outputs": [],
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "# Continue with regular imports\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "Force_Redownload = 1\n",
        "try:\n",
        "    if Force_Redownload: raise Exception(\"Force Redownload\")\n",
        "    from CSE203B_Final_Project import engine\n",
        "    from CSE203B_Final_Project.helper_functions import set_seeds, plot_loss_curves\n",
        "    from CSE203B_Final_Project.data_prep import prepare_loaders\n",
        "    from CSE203B_Final_Project.models import SViT, prepare_model\n",
        "except Exception as E:\n",
        "    print(\"[INFO] Downloading Code from GitHub.\")\n",
        "    !rm -rf CSE203B_Final_Project\n",
        "    !git clone https://github.com/ArmanOmmid/CSE203B_Final_Project\n",
        "    !mv CSE203B_Final_Project .\n",
        "finally:\n",
        "    from CSE203B_Final_Project import engine\n",
        "    from CSE203B_Final_Project.helper_functions import set_seeds, plot_loss_curves\n",
        "    from CSE203B_Final_Project.data_prep import prepare_loaders\n",
        "    from CSE203B_Final_Project.models import SViT, prepare_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device\n",
        "data_folder = 'datasets'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VdC7hFtS1_5"
      },
      "outputs": [],
      "source": [
        "# Create image size (from Table 3 in the ViT paper) ViT Paper used 224 ; # Create transform pipeline manually\n",
        "IMG_SIZE = 224 \n",
        "example_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x.repeat(3, 1, 1) if x.size(0)==1 else x)), # Turns to RGB\n",
        "])           \n",
        "data_folder = 'datasets'\n",
        "\n",
        "BATCH_SIZE = 3\n",
        "NUM_WORKERS = 2\n",
        "total_images = 25000\n",
        "test_proportion = 0.2\n",
        "dataset_name = \"Caltech256\"\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = prepare_loaders(data_folder, dataset_name, total_images, test_proportion, example_transforms, BATCH_SIZE, NUM_WORKERS)\n",
        "print(len(train_dataloader.dataset.indices), len(test_dataloader.dataset.indices), len(class_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfOkRAQoS517"
      },
      "outputs": [],
      "source": [
        "# Plot image with matplotlib\n",
        "image_batch, label_batch = next(iter(train_dataloader)) # Get a batch of images\n",
        "image, label = image_batch[0], label_batch[0] # Get a single image from the batch\n",
        "image.shape, label # View the batch shapes\n",
        "plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -> [height, width, color_channels]\n",
        "plt.title(class_names[label])\n",
        "plt.axis(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jT8ON8wjTUKW"
      },
      "outputs": [],
      "source": [
        "# Pretrained ViT (Standard)\n",
        "\n",
        "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" means best available\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=vit_weights)\n",
        "\n",
        "IMG_SIZE = 224 \n",
        "vit_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x.repeat(3, 1, 1) if x.size(0)==1 else x)), # Turns to RGB\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "total_images = 25000\n",
        "test_proportion = 0.1\n",
        "dataset_name = \"Caltech256\"\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = prepare_loaders(data_folder, dataset_name, total_images, test_proportion, vit_transforms, BATCH_SIZE, NUM_WORKERS)\n",
        "print(len(train_dataloader.dataset.indices), len(test_dataloader.dataset.indices), len(class_names))\n",
        "\n",
        "pretrained_vit, model_summary = prepare_model(pretrained_vit, len(class_names))\n",
        "# print(model_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "q_VFnVe4TY2P"
      },
      "outputs": [],
      "source": [
        "# Train a pretrained ViT feature extractor\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "set_seeds()\n",
        "\n",
        "epochs = 20\n",
        "pretrained_vit_results = engine.train(model=pretrained_vit, train_dataloader=train_dataloader, test_dataloader=test_dataloader,\n",
        "                                      optimizer=optimizer, loss_fn=loss_fn, epochs=epochs, device=device)\n",
        "\n",
        "print(\"\")\n",
        "plot_loss_curves(pretrained_vit_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bRfnngJwfYNT"
      },
      "outputs": [],
      "source": [
        "svit_backbone_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # \"DEFAULT\" means best available\n",
        "\n",
        "IMG_SIZE = 224 \n",
        "vit_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: (x.repeat(3, 1, 1) if x.size(0)==1 else x)), # Turns to RGB\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "total_images = 25000\n",
        "test_proportion = 0.1\n",
        "dataset_name = \"Caltech256\"\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = prepare_loaders(data_folder, dataset_name, total_images, test_proportion, vit_transforms, BATCH_SIZE, NUM_WORKERS)\n",
        "print(len(train_dataloader.dataset.indices), len(test_dataloader.dataset.indices), len(class_names))\n",
        "\n",
        "# Pretrained SVM-ViT (Standard)\n",
        "svit_backbone = torchvision.models.vit_b_16(weights=svit_backbone_weights)\n",
        "svit_backbone, model_summary = prepare_model(svit_backbone)\n",
        "\n",
        "svit = SViT(svit_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XLm2mbtGfba1"
      },
      "outputs": [],
      "source": [
        "fit_results = svit.fit(train_dataloader)\n",
        "score_results = svit.score(test_dataloader)\n",
        "score_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7azIMhkTkGd"
      },
      "outputs": [],
      "source": [
        "# Pretrained ViT (SWAG)\n",
        "\n",
        "vit_weights_swag = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1 # get SWAG weights\n",
        "pretrained_vit_swag = torchvision.models.vit_b_16(weights=vit_weights_swag)\n",
        "\n",
        "swag_transforms = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.convert('RGB')), # Turns to RGB\n",
        "    vit_weights_swag.transforms()\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "total_images = 25000\n",
        "test_proportion = 0.125\n",
        "dataset_name = \"Caltech256\"\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = prepare_loaders(data_folder, dataset_name, total_images, test_proportion, swag_transforms, BATCH_SIZE, NUM_WORKERS)\n",
        "print(len(train_dataloader.dataset.indices), len(test_dataloader.dataset.indices), len(class_names))\n",
        "\n",
        "pretrained_vit_swag, model_summary = prepare_model(pretrained_vit_swag, len(class_names))\n",
        "# print(model_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "nu0nL6Z0TnWd"
      },
      "outputs": [],
      "source": [
        "# Train a pretrained ViT feature extractor with SWAG weights\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit_swag.parameters(), lr=1e-3)\n",
        "loss_fn = torch.nn.CrossEntropyLoss() \n",
        "set_seeds()\n",
        "\n",
        "epochs = 20\n",
        "pretrained_vit_swag_results = engine.train(model=pretrained_vit_swag, train_dataloader=train_dataloader, test_dataloader=test_dataloader,\n",
        "                                      optimizer=optimizer, loss_fn=loss_fn, epochs=epochs, device=device)\n",
        "\n",
        "print(\"\")\n",
        "plot_loss_curves(pretrained_vit_swag_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kVVSx-k1vot0"
      },
      "outputs": [],
      "source": [
        "svit_swag_backbone_weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1 # get SWAG weights\n",
        "\n",
        "swag_transforms = transforms.Compose([\n",
        "    transforms.Lambda(lambda x: x.convert('RGB')), # Turns to RGB\n",
        "    svit_swag_backbone_weights.transforms()\n",
        "])\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 2\n",
        "total_images = 25000\n",
        "test_proportion = 0.125\n",
        "dataset_name = \"Caltech256\"\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = prepare_loaders(data_folder, dataset_name, total_images, test_proportion, swag_transforms, BATCH_SIZE, NUM_WORKERS)\n",
        "print(len(train_dataloader.dataset.indices), len(test_dataloader.dataset.indices), len(class_names))\n",
        "\n",
        "# Pretrained SVM-ViT (\u0010SWAG)\n",
        "svit_swag_backbone = torchvision.models.vit_b_16(weights=svit_swag_backbone_weights)\n",
        "svit_swag_backbone, model_summary = prepare_model(svit_swag_backbone)\n",
        "\n",
        "svit_swag = SViT(svit_swag_backbone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BfITHhzryYwU"
      },
      "outputs": [],
      "source": [
        "fit_results = svit_swag.fit(train_dataloader)\n",
        "score_results = svit_swag.score(test_dataloader)\n",
        "score_results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
